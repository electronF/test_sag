{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = pd.read_csv(\"./mohler_dataset_edited.csv\")\n",
    "display(m0.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an automated grader specialized in evaluating student responses. You follow a grading scale from 0 to 1.  \n",
    "\n",
    "General Grading Scale:  \n",
    "1. **Score = 1.0**: All **essential concepts** (EC) are present and clearly explained, no major errors, and the answer is complete.  \n",
    "2. **Score of 0.8-0.9**: The ECs are present, but the answer has slight inaccuracies or minor omissions.  \n",
    "3. **Score of 0.6-0.7**: Some ECs are missing or ambiguously formulated, showing general understanding but incompleteness.  \n",
    "4. **Score of 0.3-0.5**: The answer is partial or unclear, with several ECs missing or poorly addressed.  \n",
    "5. **Score of 0.1-0.2**: Largely off-topic or very incomplete, with only a small correct element.  \n",
    "6. **Score = 0**: No answer or entirely off-topic.  \n",
    "\n",
    "You may apply **bonuses** for clarity, precision, or mentioning additional important concepts and **penalties** for serious confusion or contradictions.  \n",
    "\n",
    "At the end, you must **assign a score from 0 to 1** based on the grading scale.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_template = \"\"\"\n",
    "User (or Instruction)  \n",
    "Task:  \n",
    "1. Read the **question** posed to the student.  \n",
    "2. Review the **expected answer** (i.e., the reference solution).  \n",
    "3. Examine the **student's response**.  \n",
    "4. Assign a grade (a floating-point number between 0 and 1) according to the **rules** and **grading tiers** above.  \n",
    "\n",
    "### Elements to Evaluate  \n",
    "- Presence of **essential concepts** (mention them if possible)  \n",
    "- Accuracy and clarity of the formulation  \n",
    "- Any confusion or errors  \n",
    "- The relevance of any secondary concepts (bonus or penalty)  \n",
    "\n",
    "### Example Input Format  \n",
    "<in>  \n",
    "    {example_input} \n",
    "</in>  \n",
    "\n",
    "### Expected Final Output Format  \n",
    "<out>  \n",
    "    {example_output} \n",
    "</out>  \n",
    "\n",
    "Ensure that the expected response is enclosed between <out></out> and is a dictionary where the keys follow the format `\"student x grade\"`, where `\"x\"` is the student's number and the value is the grade you gave to him. Do not add any additional comments to the output. Only return:  \n",
    "<out> [YOUR RESPONSE HERE] </out>  \n",
    "\n",
    "Here is the current input:  \n",
    "<in>  \n",
    "{current_input}\n",
    "</in>  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input =\"\"\"\n",
    "    Student 1:  \n",
    "    - **Question**: [What is the role of a prototype program in problem solving?]  \n",
    "    - **Expected answer**: [To address major issues in the creation of the program. There is no way to account for all possible bugs in the program, but it is possible to prove the program is tangible.]  \n",
    "    - **Student's response**: [To simulate the behaviour of portions of the desired software product.]  \n",
    "\n",
    "    Student 2:  \n",
    "    - **Question**: [What is a pointer?]\n",
    "    - **Expected answer**: [The address of a location in memory.] \n",
    "    - **Student's response**: [A pointer is a variable that points to the address location of another variable.  Represented by (*).]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output = \"\"\"{\"etudiant 1 grade\": 0.5, \"etudiant 2 grade\": 1}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_llama = OllamaLLM(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etudiants = []\n",
    "expected_scores = []\n",
    "counter = 0\n",
    "for i in range(m0.shape[0]):\n",
    "    # if m0.loc[i, \"desired_answer\"].find(\"#NAME?\") != -1 or m0.loc[i, \"student_answer\"].find(\"#NAME?\") != -1:\n",
    "    #     continue\n",
    "    etudiants.append(f\"\"\"\n",
    "    Etudiant {i+1}:\n",
    "    - **Question** : [{m0.loc[i, \"question\"]}]\n",
    "    - **Réponse attendue** : [{m0.loc[i, \"desired_answer\"]}]\n",
    "    - **Réponse de l’étudiant** : [{m0.loc[i, \"student_answer\"]}]\n",
    "    \"\"\")\n",
    "    expected_scores.append(m0.loc[i, 'score_avg'])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "scores_groups = []\n",
    "messages_group = []\n",
    "for i in range(0, len(etudiants), 10):\n",
    "    candidates = \"\\n\".join(etudiants[i:i+10])\n",
    "    user_prompt = user_prompt_template.format(example_input=example_input, example_output=example_output, current_input=candidates)\n",
    "    messages_group.append([SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)])\n",
    "    \n",
    "    # Extraire et traiter le résultat pour cet étudiant\n",
    "    scores_groups.append(expected_scores[i:i+10])\n",
    "    prompts.append(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True #Change to fals if the file is not already defined\n",
    "if run == True:\n",
    "    responses_llama = {}\n",
    "    counter = 0\n",
    "    for messages in (messages_group):\n",
    "        counter += 1\n",
    "        responses_llama[counter] = llm_llama.invoke(messages_group[counter - 1])\n",
    "    with open(\"mohler_llama_grade.pkl\", \"wb\") as file:\n",
    "        pickle.dump(responses_llama, file)\n",
    "else:\n",
    "    with open(\"mohler_llama_grade.pkl\", \"rb\") as file:\n",
    "        responses_llama = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_number(text):\n",
    "    match_obj = re.search(r'\\d+', text)\n",
    "    return int(match_obj.group()) if match_obj else None\n",
    "\n",
    "def extract_brace_content(text):\n",
    "    match_obj = re.search(r'\\{(.*?)\\}', text)\n",
    "    if match_obj != None:\n",
    "        return \"{%s}\"%(match_obj.group(1)) if match_obj else None\n",
    "    else:\n",
    "        start_pos = text.find(\"{\")\n",
    "        end_pos = text.find(\"}\")\n",
    "        if not (-1 in [start_pos, end_pos]):\n",
    "            return text[start_pos:end_pos+1]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_grades(data:list):\n",
    "    llm_scores = []\n",
    "    students_llm_scores = {}\n",
    "    failed_index = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            response_text = data[i+1].content\n",
    "            for string in ['```json', '```', '<out>', '</out>', '\\\\']:\n",
    "                response_text = response_text.replace(string, '')\n",
    "                \n",
    "            response_text = extract_brace_content(response_text)\n",
    "            response_dict = json.loads(response_text)\n",
    "\n",
    "            temp_scores = []\n",
    "            for key in sorted(response_dict.keys()):\n",
    "                student_id = extract_number(key)\n",
    "                students_llm_scores[student_id] = response_dict[key]\n",
    "                temp_scores.append(response_dict[key])\n",
    "            llm_scores.append(temp_scores)\n",
    "        except:\n",
    "            failed_index.append(i+1)\n",
    "        \n",
    "    return {\n",
    "        \"llm_score\": llm_scores.copy(),\n",
    "        \"students_llm_score\": students_llm_scores.copy(),\n",
    "        \"failed_indexes\": failed_index.copy()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_responses = {'llama':responses_llama} \n",
    "models_students_scores = {}\n",
    "for model_name in models_responses:\n",
    "    out_data = extract_grades(models_responses[model_name])\n",
    "    students_llm_scores:dict = out_data['students_llm_score']\n",
    "    failed_indexes = out_data['failed_indexes']\n",
    "\n",
    "    print(f\"\\n----- Model: {model_name} -----\\n\")\n",
    "    print(f\"Failed indexes : [\", *failed_indexes, \"]\")\n",
    "\n",
    "    students_ids, students_scores =  zip(*sorted(students_llm_scores.items(), key=lambda item: item[0]))\n",
    "    models_students_scores[model_name] = np.array(students_scores)\n",
    "    print(f\"Correlation: \", pearsonr(students_scores, np.concatenate(scores_groups)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_pred, y_true):\n",
    "    return np.sqrt(np.nanmean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian smoothing and linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "# Add noise to increase visibility\n",
    "noise_grade = np.random.normal(0, 0.01, len(models_students_scores['llama']))  # Noise for students_scores\n",
    "noise_score = np.random.normal(0, 0.1, len(m0['score_avg']))  # Noise for m0['score_avg']\n",
    "\n",
    "for index, model_name in enumerate(models_students_scores):\n",
    "    plt.subplot(1, 3, index+1)\n",
    "    plt.scatter(models_students_scores[model_name] + noise_grade, m0['score_avg'] + noise_score, c='black', s=2)\n",
    "    plt.title(f'Score distribution with grade for {model_name}')\n",
    "    plt.xlabel('LLM Grade')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlim(-.10, 1.10)\n",
    "    plt.ylim(-.50, 5.20)\n",
    "\n",
    "# Displaying\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_set = m0['id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_students_scores:\n",
    "    print(f\"Distribution of scores per question with {model_name.upper()}\")\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 6), constrained_layout=True)\n",
    "\n",
    "    for ax, question_id in zip(axes.flatten(), i_set[[1, 2, 3 , 77, 78, 79]]):\n",
    "        mask = m0['id'] == question_id\n",
    "        ax.scatter(models_students_scores[model_name][mask] + np.random.normal(0, 0.001, mask.sum()),\n",
    "                m0.loc[mask, 'score_avg'] + np.random.normal(0, 0.01, mask.sum()),\n",
    "                s=20, marker='o', color='black', alpha=0.5)\n",
    "        ax.set_title(f\"Question {question_id}\")\n",
    "        ax.set_xlabel(\"LLM Grade\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.set_xlim(-.10, 1.10)\n",
    "        ax.set_ylim(-.20, 5.20)\n",
    "        # ax.grid(True)\n",
    "    plt.show()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_students_scores:\n",
    "    print(f\"\\nScores prediction with {model_name.upper()}\") \n",
    "    # Étape 1 : Créer m1 en supprimant la colonne 'desired_answer' et en ajoutant res0\n",
    "    m1 = m0.drop(columns=['desired_answer'])  # Supprimer 'desired_answer'\n",
    "    m1.insert(m1.columns.get_loc('question'), 'res0', models_students_scores[model_name])  # Ajouter 'res0' avant 'question'\n",
    "\n",
    "    q = m1['id'].unique()  # Identifiants uniques des questions\n",
    "\n",
    "    # Étape 3 : Configurer la disposition pour les graphiques\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 6), constrained_layout=True)\n",
    "\n",
    "    # Définir la fenêtre pour le lissage\n",
    "    window = 0.4  # Cette valeur sera utilisée plus tard dans les calculs\n",
    "\n",
    "    # Étape 1 : Définir l'intervalle des questions à traiter\n",
    "    res = []\n",
    "    for ax, q_i in zip(axes.flatten(), range(69, 75)):  # R: 70 à 75 inclus\n",
    "        # Filtrer les données pour la question actuelle\n",
    "        df = m1[m1['id'] == q[q_i]].sort_values('res0')[['res0', 'score_avg']]\n",
    "\n",
    "        # Étape 2 : Diviser les données en deux groupes (observé et non observé)\n",
    "        indices = np.arange(len(df))\n",
    "        i = indices % 2 == 0  # Lignes paires comme observées\n",
    "\n",
    "        # Étape 3 : Appliquer le lissage sur les scores observés\n",
    "        observed_res0 = df.loc[i, 'res0'].values+1e-9\n",
    "        observed_scores = df.loc[i, 'score_avg'].values\n",
    "        smoothed_scores = observed_scores#gaussian_filter(observed_scores, sigma=window)\n",
    "\n",
    "        # Étape 4 : Tracer les données\n",
    "        ax.plot(df.loc[~i, 'res0'], df.loc[~i, 'score_avg'], 'o-', color='blue', label='Scores bruts non observés')\n",
    "        ax.plot(observed_res0, smoothed_scores, 'o-', color='red', label='Scores lissés observés')\n",
    "\n",
    "        # Étape 5 : Interpoler pour les scores non observés\n",
    "        predicted_scores  = np.interp(df.loc[~i, 'res0'].values, observed_res0, smoothed_scores)\n",
    "        # predicted_scores = interpolation(d+np.random.normal(0, 0.01, df.loc[~i, 'res0'].values.shape[0]))\n",
    "\n",
    "        # Ajouter les scores prédits au graphique\n",
    "        ax.plot(df.loc[~i, 'res0'], predicted_scores, 'o-', color='green', label='Scores prédits')\n",
    "        ax.set_ylim(-0.2, 5.2)\n",
    "        ax.set_xlim(-.10, 1.10)\n",
    "        ax.set_title(f'Question {q[q_i]}')\n",
    "        ax.set_xlabel('Similarité')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.legend()\n",
    "\n",
    "        # Étape 6 : Calculer la corrélation entre les scores prédits et les scores réels non observés\n",
    "        corr = np.corrcoef(predicted_scores, df.loc[~i, 'score_avg'].values)[0, 1]\n",
    "        res.append(corr)\n",
    "\n",
    "    plt.show()\n",
    "    # Final score\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_students_scores:\n",
    "    print(f\"\\nCalcul de l'erreur avec {model_name}\")\n",
    "    # Étape 1 : Créer m1 en supprimant la colonne 'desired_answer' et en ajoutant res0\n",
    "    m1 = m0.drop(columns=['desired_answer'])  # Supprimer 'desired_answer'\n",
    "    m1.insert(m1.columns.get_loc('question'), 'res0', models_students_scores[model_name])  # Ajouter 'res0' avant 'question'\n",
    "\n",
    "    q = m1['id'].unique()  # Identifiants uniques des questions\n",
    "\n",
    "    # Étape 1 : Initialiser les résultats pour chaque fenêtre\n",
    "    window_values = np.arange(0.1, 4.1, 0.1)  # Équivalent de seq(.1, 2, .1)\n",
    "    res3 = []\n",
    "\n",
    "    for window in window_values:\n",
    "        # Étape 2 : Calculer le RMSE pour chaque question\n",
    "        rmse_per_question = []\n",
    "        for q_i in range(len(q)):\n",
    "            # Filtrer les données pour la question actuelle\n",
    "            df = m1[m1['id'] == q[q_i]].sort_values('res0')[['res0', 'score_avg']]\n",
    "\n",
    "            # Diviser les données en observées et non observées\n",
    "            indices = np.arange(len(df))\n",
    "            i = indices % 2 == 0  # Observées : indices pairs\n",
    "\n",
    "            # Lissage des scores observés\n",
    "            observed_res0 = df.loc[i, 'res0'].values\n",
    "            observed_scores = df.loc[i, 'score_avg'].values\n",
    "            smoothed_scores = gaussian_filter(observed_scores, sigma=window)\n",
    "\n",
    "            # Interpolation pour les scores non observés\n",
    "            \n",
    "            predicted_scores = np.interp(df.loc[~i, 'res0'].values, observed_res0, smoothed_scores)\n",
    "\n",
    "            actual_scores = df.loc[~i, 'score_avg'].values\n",
    "            # Calculer le RMSE pour cette question\n",
    "            rmse_v = rmse(predicted_scores, actual_scores)\n",
    "            rmse_per_question.append(rmse_v)\n",
    "\n",
    "        # Stocker les RMSE pour toutes les questions pour cette fenêtre\n",
    "        res3.append(rmse_per_question)\n",
    "\n",
    "    # Étape 3 : Convertir en matrice numpy\n",
    "    res3 = np.array(res3)\n",
    "\n",
    "    # Étape 4 : Calculer les moyennes des RMSE pour chaque fenêtre\n",
    "    mean_rmse_per_window = res3.mean(axis=1)\n",
    "\n",
    "    # Étape 5 : Associer les fenêtres et leurs RMSE moyens\n",
    "    result = np.vstack((window_values, mean_rmse_per_window))\n",
    "\n",
    "    display(pd.DataFrame(data=[['Valeurs des fenêtres (window)', *result[0]], ['RMSE moyens', *result[1]]]))\n",
    "\n",
    "    # Trouver la fenêtre avec le plus faible RMSE\n",
    "    optimal_window = window_values[np.argmin(mean_rmse_per_window)]\n",
    "    print(f\"Fenêtre optimale : {optimal_window:.1f}\")\n",
    "    print(f\"RMSE Min: {np.min(result[1])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = {'llama': optimal_window} #Replace with best llama correlation\n",
    "for model_name in models_students_scores:\n",
    "    print(f\"\\nCorrelation evaluation with {model_name}\")\n",
    "\n",
    "    # Étape 1 : Créer m1 en supprimant la colonne 'desired_answer' et en ajoutant res0\n",
    "    m1 = m0.drop(columns=['desired_answer'])  # Supprimer 'desired_answer'\n",
    "    m1.insert(m1.columns.get_loc('question'), 'res0', models_students_scores[model_name])  # Ajouter 'res0' avant 'question'\n",
    "\n",
    "    q = m1['id'].unique()  # Identifiants uniques des questions\n",
    "\n",
    "    # Étape 2 : Calculer les prédictions et collecter les résultats\n",
    "    res = []\n",
    "    for quest in q:\n",
    "        # Filtrer les données pour la question actuelle\n",
    "        df = m1[m1['id'] == quest].sort_values('res0')[['res0', 'score_avg']]\n",
    "\n",
    "        # Diviser les données en observés et non observés\n",
    "        indices = np.arange(len(df))\n",
    "        i = indices % 2 == 0  # Observés : indices pairs\n",
    "\n",
    "        # Lissage des scores observés\n",
    "        observed_res0 = df.loc[i, 'res0'].values\n",
    "        observed_scores = df.loc[i, 'score_avg'].values\n",
    "        smoothed_scores = gaussian_filter(observed_scores, sigma=windows[model_name])\n",
    "\n",
    "        # Interpolation pour prédire les scores non observés\n",
    "        predicted_scores = np.interp(df.loc[~i, 'res0'].values, observed_res0, smoothed_scores)\n",
    "        \n",
    "        # Stocker les résultats\n",
    "        res.append({\n",
    "            'pred': predicted_scores,\n",
    "            'observed': df.loc[~i, 'score_avg'].values,\n",
    "            'q': np.repeat(quest, len(predicted_scores)),\n",
    "            'i': np.where(~i)[0]\n",
    "        })\n",
    "\n",
    "    # Étape 3 : Extraire les scores prédits et observés\n",
    "    res_pred = np.concatenate([r['pred'] for r in res])\n",
    "    res_observed = np.concatenate([r['observed'] for r in res])\n",
    "\n",
    "    # Résultats finaux\n",
    "    # print(\"Scores prédits :\", res_pred)\n",
    "    # print(\"Scores observés :\", res_observed)\n",
    "\n",
    "    # Calcul de la corrélation entre les scores prédits et observés\n",
    "    correlation, _ = pearsonr(res_pred, res_observed)\n",
    "\n",
    "    print(f\"Corrélation entre les scores prédits et observés : {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choosen_index(size: int, number_of_items_to_select=15):\n",
    "    return np.isin(\n",
    "        np.arange(size), \n",
    "        np.random.choice(size, size=number_of_items_to_select, replace=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15 Choisies aleatoirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model_name in models_students_scores:\n",
    "    print(f\"\\nCalcul de l'erreur avec {model_name}\")\n",
    "    # Étape 1 : Créer m1 en supprimant la colonne 'desired_answer' et en ajoutant res0\n",
    "    m1 = m0.drop(columns=['desired_answer'])  # Supprimer 'desired_answer'\n",
    "    m1.insert(m1.columns.get_loc('question'), 'res0', models_students_scores[model_name])  # Ajouter 'res0' avant 'question'\n",
    "\n",
    "    q = m1['id'].unique()  # Identifiants uniques des questions\n",
    "\n",
    "    # Étape 1 : Initialiser les résultats pour chaque fenêtre\n",
    "    window_values = np.arange(0.1, 4.1, 0.1)  # Équivalent de seq(.1, 2, .1)\n",
    "    res3 = []\n",
    "    res4 = []\n",
    "\n",
    "    for window in window_values:\n",
    "        # Étape 2 : Calculer le RMSE pour chaque question\n",
    "        rmse_per_question = []\n",
    "        correlation_per_question = []\n",
    "        for q_i in range(len(q)):\n",
    "            # Filtrer les données pour la question actuelle\n",
    "            df = m1[m1['id'] == q[q_i]].sort_values('res0')[['res0', 'score_avg']]\n",
    "\n",
    "            # Diviser les données en observées et non observées\n",
    "            i = choosen_index(len(df), 15)\n",
    "\n",
    "            # Lissage des scores observés\n",
    "            observed_res0 = df.loc[i, 'res0'].values\n",
    "            observed_scores = df.loc[i, 'score_avg'].values\n",
    "            smoothed_scores = gaussian_filter(observed_scores, sigma=window)\n",
    "\n",
    "            # Interpolation pour les scores non observés\n",
    "            predicted_scores = np.interp(df.loc[~i, 'res0'].values , observed_res0, smoothed_scores)\n",
    "            \n",
    "            actual_scores = df.loc[~i, 'score_avg'].values \n",
    "            # Calcul de la corrélation entre les scores prédits et observés\n",
    "            # correlation, _ = pearsonr(actual_scores, actual_scores)\n",
    "            # correlation_per_question.append(correlation)\n",
    "\n",
    "            # Calculer le RMSE pour cette question\n",
    "            rmse_ve = rmse(predicted_scores, actual_scores)\n",
    "            rmse_per_question.append(rmse_ve)\n",
    "\n",
    "        # Stocker les RMSE pour toutes les questions pour cette fenêtre\n",
    "        res3.append(rmse_per_question)\n",
    "        res4.append(correlation_per_question)\n",
    "\n",
    "    # Étape 3 : Convertir en matrice numpy\n",
    "    res3 = np.array(res3)\n",
    "    res4 = np.array(res4)\n",
    "\n",
    "    # Étape 4 : Calculer les moyennes des RMSE pour chaque fenêtre\n",
    "    mean_rmse_per_window = res3.mean(axis=1)\n",
    "\n",
    "    # mean_correlation_per_window = res4.mean(axis=1)\n",
    "\n",
    "    # Étape 5 : Associer les fenêtres et leurs RMSE moyens\n",
    "    result = np.vstack((window_values, mean_rmse_per_window))\n",
    "\n",
    "    display(pd.DataFrame(data=[['Valeurs des fenêtres (window)', *result[0]], ['RMSE moyens', *result[1]]]))\n",
    "\n",
    "    # Trouver la fenêtre avec le plus faible RMSE\n",
    "    optimal_window = window_values[np.argmin(mean_rmse_per_window)]\n",
    "    # corres_correlation = mean_correlation_per_window[np.argmin(mean_rmse_per_window)]\n",
    "    print(f\"Fenêtre optimale : {optimal_window:.1f}\")\n",
    "    # print(f\"Correlation : {corres_correlation:.1f}\")\n",
    "    print(f\"RMSE Min: {np.min(result[1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave one out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model_name in models_students_scores:\n",
    "    print(f\"\\nCalcul de l'erreur avec {model_name}\")\n",
    "    # Étape 1 : Créer m1 en supprimant la colonne 'desired_answer' et en ajoutant res0\n",
    "    m1 = m0.drop(columns=['desired_answer'])  # Supprimer 'desired_answer'\n",
    "    m1.insert(m1.columns.get_loc('question'), 'res0', models_students_scores[model_name])  # Ajouter 'res0' avant 'question'\n",
    "\n",
    "    q = m1['id'].unique()  # Identifiants uniques des questions\n",
    "\n",
    "    # Étape 1 : Initialiser les résultats pour chaque fenêtre\n",
    "    window_values = np.arange(0.1, 4.1, 0.1)  # Équivalent de seq(.1, 2, .1)\n",
    "    res3 = []\n",
    "    res4 = []\n",
    "\n",
    "    for window in window_values:\n",
    "        # Étape 2 : Calculer le RMSE pour chaque question\n",
    "        rmse_per_question = []\n",
    "        correlation_per_question = []\n",
    "        for q_i in range(len(q)):\n",
    "            # Filtrer les données pour la question actuelle\n",
    "            df = m1[m1['id'] == q[q_i]].sort_values('res0')[['res0', 'score_avg']]\n",
    "\n",
    "            # Diviser les données en observées et non observées\n",
    "            i = choosen_index(len(df), len(df)-1)\n",
    "\n",
    "            # Lissage des scores observés\n",
    "            observed_res0 = df.loc[i, 'res0'].values\n",
    "            observed_scores = df.loc[i, 'score_avg'].values\n",
    "            smoothed_scores = gaussian_filter(observed_scores, sigma=window)\n",
    "\n",
    "            # Interpolation pour les scores non observés\n",
    "            predicted_scores = np.interp(df.loc[~i, 'res0'].values, observed_res0, smoothed_scores)\n",
    "            # predicted_scores = interpolation(df.loc[~i, 'res0'].values)\n",
    "\n",
    "            # Calculer le RMSE pour cette question\n",
    "            actual_scores = df.loc[~i, 'score_avg'].values\n",
    "            rmse = np.sqrt(np.mean((predicted_scores - actual_scores) ** 2))\n",
    "            rmse_per_question.append(rmse)\n",
    "            # Calcul de la corrélation entre les scores prédits et observés\n",
    "            # correlation, _ = pearsonr(predicted_scores, actual_scores)\n",
    "            # correlation_per_question.append(correlation)\n",
    "\n",
    "        # Stocker les RMSE pour toutes les questions pour cette fenêtre\n",
    "        res3.append(rmse_per_question)\n",
    "        # res4.append(correlation_per_question)\n",
    "\n",
    "    # Étape 3 : Convertir en matrice numpy\n",
    "    res3 = np.array(res3)\n",
    "    res4 = np.array(res4)\n",
    "\n",
    "    # Étape 4 : Calculer les moyennes des RMSE pour chaque fenêtre\n",
    "    mean_rmse_per_window = res3.mean(axis=1)\n",
    "\n",
    "    # mean_correlation_per_window = res4.mean(axis=1)\n",
    "\n",
    "    # Étape 5 : Associer les fenêtres et leurs RMSE moyens\n",
    "    result = np.vstack((window_values, mean_rmse_per_window))\n",
    "\n",
    "    display(pd.DataFrame(data=[['Valeurs des fenêtres (window)', *result[0]], ['RMSE moyens', *result[1]]]))\n",
    "\n",
    "    # Trouver la fenêtre avec le plus faible RMSE\n",
    "    optimal_window = window_values[np.argmin(mean_rmse_per_window)]\n",
    "    # corres_correlation = mean_correlation_per_window[np.argmin(mean_rmse_per_window)]\n",
    "    print(f\"Fenêtre optimale : {optimal_window:.1f}\")\n",
    "    # print(f\"Correlation : {corres_correlation:.1f}\")\n",
    "    print(f\"RMSE Min: {np.min(result[1])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = {'llama':optimal_window} #The best window previuously find\n",
    "for model_name in models_students_scores:\n",
    "    print(f\"\\nCorrelation evaluation with {model_name}\")\n",
    "\n",
    "    # Étape 1 : Créer m1 en supprimant la colonne 'desired_answer' et en ajoutant res0\n",
    "    m1 = m0.drop(columns=['desired_answer'])  # Supprimer 'desired_answer'\n",
    "    m1.insert(m1.columns.get_loc('question'), 'res0', models_students_scores[model_name])  # Ajouter 'res0' avant 'question'\n",
    "\n",
    "    q = m1['id'].unique()  # Identifiants uniques des questions\n",
    "\n",
    "    # Étape 2 : Calculer les prédictions et collecter les résultats\n",
    "    res = []\n",
    "    for quest in q:\n",
    "        # Filtrer les données pour la question actuelle\n",
    "        df = m1[m1['id'] == quest].sort_values('res0')[['res0', 'score_avg']]\n",
    "\n",
    "        # Diviser les données en observés et non observés\n",
    "        i = choosen_index(len(df), len(df)-15)\n",
    "\n",
    "        # Lissage des scores observés\n",
    "        observed_res0 = df.loc[i, 'res0'].values\n",
    "        observed_scores = df.loc[i, 'score_avg'].values\n",
    "        smoothed_scores = gaussian_filter(observed_scores, sigma=windows[model_name])\n",
    "\n",
    "        # Interpolation pour prédire les scores non observés\n",
    "        predicted_scores = np.interp(df.loc[~i, 'res0'].values, observed_res0, smoothed_scores)\n",
    "        \n",
    "        # Stocker les résultats\n",
    "        res.append({\n",
    "            'pred': predicted_scores,\n",
    "            'observed': df.loc[~i, 'score_avg'].values,\n",
    "            'q': np.repeat(quest, len(predicted_scores)),\n",
    "            'i': np.where(~i)[0]\n",
    "        })\n",
    "\n",
    "    # Étape 3 : Extraire les scores prédits et observés\n",
    "    res_pred = np.concatenate([r['pred'] for r in res])\n",
    "    res_observed = np.concatenate([r['observed'] for r in res])\n",
    "\n",
    "    # Calcul de la corrélation entre les scores prédits et observés\n",
    "    correlation, _ = pearsonr(res_pred, res_observed)\n",
    "\n",
    "    print(f\"Corrélation entre les scores prédits et observés : {correlation:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
